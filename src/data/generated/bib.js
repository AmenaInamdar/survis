const generatedBibEntries = {
    "Brummernhenrich2025": {
        "abstract": "Generative AI systems like chatbots are increasingly being introduced into learning, teaching and assessment scenarios at universities. While previous research suggests that users treat chatbots like humans, computer systems are still often perceived as less trustworthy, potentially impairing their usefulness in learning contexts. How are processes of social cognition applied to chatbots compared to humans? Our study focuses on the role of politeness in communication. We hypothesise that polite communication improves the perception of trustworthiness of chatbots. University students read a feedback dialogue between a student and a feedback provider. In a 2\u2009\u00d7\u20092 between-subjects experimental design, we manipulated the feedback's author (chatbot vs. human teacher) and the feedback formulation (polite vs. direct). Participants evaluated the feedback giver on measures of epistemic trustworthiness (expertise, benevolence and integrity) and on two basic dimensions of social cognition, namely agency and communion. Results showed that a polite feedback giver was rated higher on benevolence and communion, whereas a direct feedback giver was rated higher on agency. Unexpectedly, the chatbot was rated lower on benevolence than the human. This suggests that social cognition does apply to interactions with chatbots, with caveats. We discuss the findings regarding the design of feedback chatbots and their use in higher education.",
        "author": "Brummernhenrich, Benjamin and Paulus, Christian and Jucks, Regina",
        "doi": "https://doi.org/10.1111/bjet.13569",
        "journal": "British Journal of Educational Technology",
        "keywords": "type:chatbots, generative AI",
        "number": "01",
        "publisher": "BERA",
        "title": "Applying social cognition to feedback chatbots: Enhancing trustworthiness through politeness",
        "type": "article",
        "url": "https://bera-journals.onlinelibrary.wiley.com/doi/full/10.1111/bjet.13569",
        "volume": "-",
        "year": "2025"
    },
    "Chen2024": {
        "abstract": "This paper presents a method to evaluate the alignment between the decision-making logic of Large Language Models (LLMs) and human cognition in a case study on legal LLMs. Unlike traditional evaluations on language generation results, we propose to evaluate the correctness of the detailed decision-making logic of an LLM behind its seemingly correct outputs, which represents the core challenge for an LLM to earn human trust. To this end, we quantify the interactions encoded by the LLM as primitive decision-making logic, because recent theoretical achievements [Li and Zhang, 2023, Ren et al., 2024] have proven several mathematical guarantees of the faithfulness of the interaction-based explanation. We design a set of metrics to evaluate the detailed decision-making logic of LLMs. Experiments show that even when the language generation results appear correct, a significant portion of the internal inference logic contains notable issues",
        "author": "Chen, Lu and Huang, Yuxuan and Li, Yixing and Jin, Yaohui and Zhao, Shuai and Zheng, Zilong and Zhang, Quanshi",
        "doi": "10.48550/arXiv.2410.09083",
        "journal": "-",
        "keywords": "type:\tArtificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Machine Learning",
        "number": "02",
        "publisher": "arXiv",
        "title": "Alignment Between the Decision-Making Logic of LLMs and Human Cognition: A Case Study on Legal LLMs",
        "type": "article",
        "url": "https://arxiv.org/abs/2410.09083",
        "volume": "-",
        "year": "2024"
    },
    "Laun2025": {
        "abstract": "In recent years, chatbots have emerged as potential individual assistance systems in educational settings. However, their successful implementation depends on their ability to deliver high-quality outcomes and gain students\u2019 trust. Against this background, this study investigated the perceived trustworthiness of chatbots in nursing education and whether this trust was justified in light of chatbots\u2019 actual performance in care plan creation. We conducted a study with 189 vocational nursing students who created medical care plans under two conditions: without ChatGPT (GPT-4) and with ChatGPT assistance. Additionally, ChatGPT was used to develop care plans without student involvement. Expert evaluations of these care plans allowed us to compare quality across conditions. We also examined students\u2019 trust by having them evaluate another care plan before and after experimentally manipulating information about its source (chatbot or peer-generated). Statistical analyses revealed that source disclosure significantly affected students\u2019 trust: Care plans believed to be chatbot-generated experienced a significant decrease in trust, while those attributed to peers showed no significant change. However, analyses of expert evaluations showed that ChatGPT-generated plans were of higher quality than those created by students, even when students used ChatGPT assistance. This discrepancy between perceived trustworthiness and actual performance of chatbots indicates that students\u2019 skepticism toward chatbot-generated content is, to some extent, exaggerated. While ChatGPT can enhance students\u2019 care plan quality, our findings emphasize the importance of addressing trust issues in educational settings where chatbots are implemented.",
        "author": "Laun, Martin and Puderbach, Leonard and Hirt, Katharina and Wyss, Eva and Friemert, Daniel and Hartman, Ulrich and Wolf, Fabian",
        "doi": "https://doi.org/10.1016/j.cedpsych.2025.102373",
        "journal": "Contemporary Educational Psychology",
        "keywords": "type:Chatbots, Education, Trustworthiness",
        "number": "03",
        "publisher": "Elsevier",
        "title": "Chatbots in education: Outperforming students but perceived as less trustworthy",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S0361476X25000384",
        "volume": "81",
        "year": "2025"
    },
    "Luo2024": {
        "abstract": "Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. Here, to evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs indicated high confidence in their predictions, their responses were more likely to be correct, which presages a future where LLMs assist humans in making discoveries. Our approach is not neuroscience specific and is transferable to other knowledge-intensive endeavours.",
        "author": "Lou, Xiaoliang and Rechardt, Akilles and Sun, Guangzhi and Nejad, Kevin and Y\u00e1\u00f1ez, Felipe",
        "doi": "https://doi.org/10.1038/s41562-024-02046-9",
        "journal": "Nature Human Behaviour",
        "keywords": "type:LLMs, Neuroscience",
        "number": "04",
        "pages": "305-315",
        "publisher": "Nature",
        "title": "Large language models surpass human experts in predicting neuroscience results",
        "type": "article",
        "url": "https://www.nature.com/articles/s41562-024-02046-9#citeas",
        "volume": "9",
        "year": "2024"
    },
    "Ong2024": {
        "abstract": "With the rapid growth of interest in and use of large language models (LLMs) across various industries, we are facing some crucial and profound ethical concerns, especially in the medical field. The unique technical architecture and purported emergent abilities of LLMs differentiate them substantially from other artificial intelligence (AI) models and natural language processing techniques used, necessitating a nuanced understanding of LLM ethics. In this Viewpoint, we highlight ethical concerns stemming from the perspectives of users, developers, and regulators, notably focusing on data privacy and rights of use, data provenance, intellectual property contamination, and broad applications and plasticity of LLMs. A comprehensive framework and mitigating strategies will be imperative for the responsible integration of LLMs into medical practice, ensuring alignment with ethical principles and safeguarding against potential societal risks.",
        "author": "Ong, Jasmine and Chang, Shelley and Wasswa, Willian and Butte, Atul and Shah, Nigam and Chew, Lita",
        "doi": "10.1016/S2589-7500(24)00061-X",
        "issue": "6",
        "journal": "The Lancet, Digital Health",
        "keywords": "type:LLMs, ethical, regulatory",
        "number": "05",
        "pages": "428-432",
        "publisher": "The Lancet",
        "title": "Ethical and regulatory challenges of large language models in medicine",
        "type": "article",
        "url": "https://www.thelancet.com/journals/landig/article/PIIS2589-7500(24)00061-X/fulltext",
        "volume": "6",
        "year": "2024"
    },
    "Schlicker2025": {
        "abstract": "This research advances trust theory by examining factors shaping the development of a trustor\u2019s perceived trustworthiness in the context of real-world interactions with a large language model-driven virtual doctor (VD). Employing a qualitative approach to elaborate the trustworthiness assessment model, we conducted 51 interviews with 65 participants. Our findings reveal a het\u0002erogeneity in the trustworthiness perceptions of and reported trust in VDs, ranging from a complete absence to a complete presence of trust, with many participants expressing conditional trust. The key factors contributing to this heterogeneity were participants\u2019 benchmarks for trustworthiness, na\u00efve theories, risk\u2013benefit assessments, individual standards, and strategies for cue detection and utilization in assessing the trustworthiness of the VD. Our findings also highlight the crucial influence of third-party involvement in artificial intelligence system development and testing on trustworthiness assessments. These insights underscore the trustworthiness assessment model\u2019s utility in understanding trust development processes",
        "author": "Schlicker, Nadine and Lechner, Fabian and Wehrle, Katja and Greulich, Berit and Hirsch, Martin and Langer, Markus",
        "doi": "https://doi.org/10.1037/tmb0000164",
        "issue": "2",
        "journal": "Technology, Mind and Behaviour",
        "keywords": "type: large language models, trust in artificial intelligence, trustworthiness assessment, artificial intelligence in medicine",
        "number": "06",
        "publisher": "TMB",
        "title": "Trustworthy Enough? Examining Trustworthiness Assessments of Large Language Model-Based Medical Agents",
        "type": "article",
        "url": "https://assets.pubpub.org/0wfgaemk/tmb_tmb0000164-41746448610539.pdf",
        "volume": "6",
        "year": "2025"
    },
    "Stade2024": {
        "abstract": "Large language models (LLMs) such as Open AI\u2019s GPT-4 (which power ChatGPT) and Google\u2019s Gemini, built on artificial intelligence, hold immense potential to support, augment, or even eventually automate psychotherapy. Enthusiasm about such applications is mounting in the field as well as industry. These developments promise to address insufficient mental healthcare system capacity and scale individual access to personalized treatments. However, clinical psychology is an uncommonly high stakes application domain for AI systems, as responsible and evidence-based therapy requires nuanced expertise. This paper provides a roadmap for the ambitious yet responsible application of clinical LLMs in psychotherapy. First, a technical overview of clinical LLMs is presented. Second, the stages of integration of LLMs into psychotherapy are discussed while highlighting parallels to the development of autonomous vehicle technology. Third, potential applications of LLMs in clinical care, training, and research are discussed, highlighting areas of risk given the complex nature of psychotherapy. Fourth, recommendations for the responsible development and evaluation of clinical LLMs are provided, which include centering clinical science, involving robust interdisciplinary collaboration, and attending to issues like assessment, risk detection, transparency, and bias. Lastly, a vision is outlined for how LLMs might enable a new generation of studies of evidence-based interventions at scale, and how these studies may challenge assumptions about psychotherapy.",
        "author": "Stade, Elizabeth and Stirman, Shannon and Ungar, Lyle and Boland, Cody and Schwartz, H and Yaden, David and Sedoc, Jo\u00e3o and DeRubeis, Robert and Willier, Robb and Eichstaedt, Johannes ",
        "doi": "https://doi.org/10.1038/s44184-024-00056-z",
        "journal": "NPJ Mental Health Research",
        "keywords": "type:LLMs, artificial intelligence, GPT-4, behavioural healthcare, evaluation",
        "number": "07",
        "publisher": "Nature",
        "title": "Large language models could change the future of behavioral healthcare: a proposal for responsible development and evaluation",
        "type": "article",
        "url": "https://www.nature.com/articles/s44184-024-00056-z#citeas",
        "volume": "03",
        "year": "2024"
    },
    "Xu2024": {
        "abstract": "Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.",
        "author": "Xu, Xuhai and Yao, Bingsheng and Dong, Yuanzhe and Gabriel, Saadi and Yu, Hong and Hendler, James and Ghassemi, Marzyeh and Dey, Anind and Wang, Dakuo",
        "doi": "https://doi.org/10.1145/3643540",
        "journal": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "keywords": "type:LLMs, mental health, prediction",
        "number": "08",
        "pages": "01-32",
        "publisher": "ACM Journals",
        "title": "Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data",
        "type": "article",
        "url": "https://dl.acm.org/doi/pdf/10.1145/3643540",
        "volume": "8",
        "year": "2024"
    },
    "Yuan2025": {
        "abstract": "The global rise in mental disorders, particularly in workplaces, necessitated innovative and scalable solutions for delivering therapy. Large Language Model (LLM)-based mental health chatbots have rapidly emerged as a promising tool for overcoming the time, cost, and accessibility constraints often associated with traditional mental health therapy. However, LLM-based mental health chatbots are in their nascency, with significant opportunities to enhance their capabilities to operate within organizational contexts. To this end, this research seeks to examine the role and development of LLMs in mental health chatbots over the past half-decade. Through our review, we identified over 50 mental health-related chatbots, including 22 LLM-based models targeting general mental health, depression, anxiety, stress, and suicide ideation. These chatbots are primarily used for emotional support and guidance but often lack capabilities specifically designed for workplace mental health, where such issues are increasingly prevalent. The review covers their development, applications, evaluation, ethical concerns, integration with traditional services, LLM-as-a-Service, and various other business implications in organizational settings. We provide a research illustration of how LLM-based approaches could overcome the identified limitations and also offer a system that could help facilitate systematic evaluation of LLM-based mental health chatbots. We offer suggestions for future research tailored to workplace mental health needs.",
        "author": "Yuan, Aijia and Colato, Edlin and Pescosolido, Bernice and Song, Hyunju and Samtani, Sagar",
        "doi": "https://doi.org/10.1145/3701041",
        "journal": "ACM Transactions on Management Information Systems",
        "keywords": "type: chatbots, mental health, LLMs",
        "number": "09",
        "pages": "01-26",
        "publisher": "ACM Journals",
        "title": "Improving Workplace Well-being in Modern Organizations: A Review of Large Language Model-based Mental Health Chatbots",
        "type": "article",
        "url": "https://dl.acm.org/doi/full/10.1145/3701041",
        "volume": "16",
        "year": "2025"
    },
    "Zylowski2024": {
        "abstract": "Advising and supporting students in their career choices and the associated further education paths is becoming increasingly important. Current studies show that there is a high drop-out rate in Bachelor\u2018s degree courses, which is due to the fact that the ideas and expectations of the students and the content offered in the course do not overlap sufficiently. While pupils widely started to use ChatGPT as a tool for education, it is not specifically a career-counselling system, lacking domain knowledge and country-specific data. Another drawback is that there is no guidance in the conversation and a conversation is driven by the users questions, not by the system itself. To meet this challenge, we have developed an AI-based assistant to provide individual advice on career choices and identify suitable educational paths. The assistant is a chatbot based on a modern open-source large language model (LLM) hosted in a data-sovereign manner and adapted to the educational domain using prompt engineering. This allows for an open conversation that is more human-like. In a conversation, the goals, ideas, experiences and abilities of the students are determined and a user profile is created. This user profile is used to make several recommendations for an occupation. If the user is not satisfied, the conversation is deepened further for more details. As soon as an occupation matches the student\u2018s requirements, several possible educational paths are calculated. These can consist of apprenticeships and different bachelor and master courses leading to the selected occupation and adapted to the student\u2018s profile.",
        "author": "Zylowski, T and Patricio, N and Hettmann, W and Anderer, K and Wolfel, M and Henning, P",
        "doi": "10.21125/edulearn.2024.1528",
        "journal": "Edulearn24 Proceedings",
        "keywords": "type:Generative Artificial Intelligence, User Study, Conversational User Interface, Chatbot, Educational Pathway Recommendation, Large Language Model",
        "number": "10",
        "pages": "6461-6470",
        "publisher": "IATED",
        "title": "Evaluating Trustworthiness, Usability and Explainability of an Educational Pathway Recommendation System that uses a large language model",
        "type": "article",
        "url": "https://library.iated.org/view/ZYLOWSKI2024EVA",
        "volume": "-",
        "year": "2024"
    }
};